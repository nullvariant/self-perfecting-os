#!/usr/bin/env python3
"""
ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨çŽ‡ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

VS Code Copilot Chatã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨çŠ¶æ³ã‚’æŽ¨å®šã—ã€è­¦å‘Šã‚’å‡ºåŠ›ã™ã‚‹ã€‚

Usage:
    python scripts/check_token_usage.py
    python scripts/check_token_usage.py --detailed
"""

import argparse
import re
import sys
from pathlib import Path
from typing import Any, Iterable, Optional

try:
    import yaml
except ImportError:  # pragma: no cover - optional dependency
    yaml = None


PROJECT_ROOT = Path(__file__).resolve().parent.parent
CONVERSATION_DIR_CANDIDATES = [
    PROJECT_ROOT / "conversations",
    PROJECT_ROOT / "docs" / "log",
]
LOG_FILE_EXTENSIONS = {".md", ".markdown", ".json", ".yaml", ".yml", ".txt"}
TOTAL_TOKEN_PATTERN = re.compile(
    r"total[\s_-]*tokens?(?:\s*[:=]\s*|\s+[^\d]*)(\d[\d,]*)",
    re.IGNORECASE,
)
GENERIC_TOKEN_PATTERN = re.compile(
    r"tokens?(?:_total|_used|_usage|[\s_-]*(?:ä½¿ç”¨é‡|åˆè¨ˆ|æ¶ˆè²»))?\s*[:=]?\s*(\d[\d,]*)",
    re.IGNORECASE,
)

# å®šæ•°
TOKEN_LIMIT = 1_000_000  # VS Code Copilot Chatã®ä¸Šé™
WARNING_THRESHOLD = 0.60  # 60%ã§è­¦å‘Š
CRITICAL_THRESHOLD = 0.80  # 80%ã§ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆ

# æŽ¨å®šå€¤ï¼ˆç¾åœ¨ã®å¯¾è©±ã‹ã‚‰ç®—å‡ºï¼‰
# 2025-10-29æ™‚ç‚¹: ç´„79,000ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨ï¼ˆè¦ç´„å‰ï¼‰
ESTIMATED_CURRENT_USAGE = 79_000


def _coerce_int(value: Any) -> Optional[int]:
    """Convert supported values to int, ignoring booleans and invalid data."""
    if isinstance(value, bool):
        return None
    if isinstance(value, (int, float)):
        return int(value)
    if isinstance(value, str):
        digits = re.sub(r"[^\d]", "", value)
        if digits:
            return int(digits)
    return None


def _extract_tokens_from_mapping(data: Any) -> Optional[int]:
    totals = []
    partials = []
    stack: list[tuple[str, Any]] = [("", data)]

    while stack:
        path, node = stack.pop()
        if isinstance(node, dict):
            for key, value in node.items():
                key_str = str(key)
                next_path = f"{path}.{key_str}" if path else key_str
                stack.append((next_path, value))
        elif isinstance(node, list):
            for item in node:
                stack.append((path, item))
        else:
            numeric = _coerce_int(node)
            if numeric is None or not path:
                continue
            path_lower = path.lower()
            if "token" not in path_lower:
                continue
            if "total" in path_lower:
                totals.append(numeric)
            else:
                partials.append(numeric)

    if totals:
        return sum(totals)
    if partials:
        return sum(partials)
    return None


def _extract_frontmatter(text: str) -> Optional[str]:
    lines = text.splitlines()
    if not lines or lines[0].strip() != "---":
        return None
    for idx in range(1, len(lines)):
        if lines[idx].strip() == "---":
            return "\n".join(lines[1:idx])
    return None


def _extract_tokens_from_text(text: str) -> Optional[int]:
    values = [
        _coerce_int(match.group(1))
        for match in TOTAL_TOKEN_PATTERN.finditer(text)
    ]
    totals = [value for value in values if value is not None]
    if totals:
        return sum(totals)

    values = [
        _coerce_int(match.group(1))
        for match in GENERIC_TOKEN_PATTERN.finditer(text)
    ]
    generic = [value for value in values if value is not None]
    if generic:
        return sum(generic)

    return None


def _extract_tokens_from_frontmatter(text: str) -> Optional[int]:
    if not text:
        return None
    if yaml is not None:
        try:
            data = yaml.safe_load(text)
        except yaml.YAMLError:
            data = None
        if data is not None:
            usage = _extract_tokens_from_mapping(data)
            if usage is not None:
                return usage
    return _extract_tokens_from_text(text)


def _extract_usage_from_file(path: Path) -> int:
    try:
        contents = path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        contents = path.read_text(encoding="utf-8", errors="ignore")

    frontmatter = _extract_frontmatter(contents)
    if frontmatter:
        usage = _extract_tokens_from_frontmatter(frontmatter)
        if usage is not None:
            return usage

    usage = _extract_tokens_from_text(contents)
    return usage or 0


def _iter_conversation_log_files() -> Iterable[Path]:
    seen = set()
    for candidate in CONVERSATION_DIR_CANDIDATES:
        if not candidate.exists():
            continue
        for path in candidate.rglob("*"):
            if not path.is_file():
                continue
            if path.suffix.lower() not in LOG_FILE_EXTENSIONS:
                continue
            resolved = path.resolve()
            if resolved in seen:
                continue
            seen.add(resolved)
            yield resolved


def estimate_token_usage():
    """
    ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æŽ¨å®š
    
    Returns:
        int: æŽ¨å®šãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
    
    Note:
        ä¼šè©±ãƒ­ã‚°ã‹ã‚‰ç®—å‡ºã§ããªã„å ´åˆã¯ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ã¨ã—ã¦æŽ¨å®šå€¤ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã€‚
    """
    total_usage = 0
    observed_files = False

    for log_file in _iter_conversation_log_files():
        observed_files = True
        total_usage += _extract_usage_from_file(log_file)

    if observed_files and total_usage > 0:
        return total_usage

    return ESTIMATED_CURRENT_USAGE


def calculate_usage_percentage(current_usage: int) -> float:
    """ä½¿ç”¨çŽ‡ã‚’è¨ˆç®—"""
    return (current_usage / TOKEN_LIMIT) * 100


def estimate_remaining_conversations(current_usage: int) -> int:
    """
    æ®‹ã‚Šå¯¾è©±å¯èƒ½å›žæ•°ã‚’æŽ¨å®š
    
    ä»®å®š: 1å›žã®å¯¾è©±ã§å¹³å‡3,000ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨
    """
    avg_tokens_per_conversation = 3_000
    remaining_tokens = TOKEN_LIMIT - current_usage
    return remaining_tokens // avg_tokens_per_conversation


def get_status_emoji(usage_percentage: float) -> str:
    """ä½¿ç”¨çŽ‡ã«å¿œã˜ãŸçµµæ–‡å­—ã‚’è¿”ã™"""
    if usage_percentage >= CRITICAL_THRESHOLD * 100:
        return "ðŸš¨"  # ç·Šæ€¥
    elif usage_percentage >= WARNING_THRESHOLD * 100:
        return "âš ï¸"  # è­¦å‘Š
    else:
        return "âœ…"  # å®‰å…¨


def print_usage_report(current_usage: int, detailed: bool = False):
    """ä½¿ç”¨çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
    usage_percentage = calculate_usage_percentage(current_usage)
    remaining_conversations = estimate_remaining_conversations(current_usage)
    status_emoji = get_status_emoji(usage_percentage)
    
    print("=" * 60)
    print("ðŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨çŽ‡ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    print()
    print(f"{status_emoji} ç¾åœ¨ã®ä½¿ç”¨çŽ‡: {usage_percentage:.1f}%")
    print(f"   ä½¿ç”¨é‡: {current_usage:,} / {TOKEN_LIMIT:,} tokens")
    print(f"   æ®‹é‡: {TOKEN_LIMIT - current_usage:,} tokens")
    print()
    print(f"ðŸ“ˆ æ®‹ã‚Šå¯¾è©±å¯èƒ½å›žæ•°ï¼ˆæŽ¨å®šï¼‰: ç´„{remaining_conversations}å›ž")
    print()
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
    if usage_percentage >= CRITICAL_THRESHOLD * 100:
        print("ðŸš¨ ã€ç·Šæ€¥ã€‘ 80%è¶…éŽï¼")
        print("   æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("   1. é‡è¦ãªå¯¾è©±ã®ã¿ã«çµžã‚‹")
        print("   2. ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜ã—ã€æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚’æ¤œè¨Ž")
        print("   3. å¯¾è©±ãƒ­ã‚°ã‚’å³åº§ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")
    elif usage_percentage >= WARNING_THRESHOLD * 100:
        print("âš ï¸  ã€è­¦å‘Šã€‘ 60%è¶…éŽ")
        print("   æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("   1. é‡è¦ãªå¯¾è©±ã‚’å„ªå…ˆã™ã‚‹")
        print("   2. è¿‘æ—¥ä¸­ã«æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚’æ¤œè¨Ž")
        print("   3. å¯¾è©±ãƒ­ã‚°ã®å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ç¢ºèª")
    else:
        print("âœ… å®‰å…¨ç¯„å›²å†…ï¼ˆ60%æœªæº€ï¼‰")
        print("   ç¾åœ¨ã®ãƒšãƒ¼ã‚¹ã§ä½¿ç”¨ã‚’ç¶™ç¶šã§ãã¾ã™ã€‚")
    
    print()
    
    if detailed:
        print("=" * 60)
        print("ðŸ“‹ è©³ç´°æƒ…å ±")
        print("=" * 60)
        print(f"è­¦å‘Šé–¾å€¤: {WARNING_THRESHOLD * 100:.0f}% ({int(TOKEN_LIMIT * WARNING_THRESHOLD):,} tokens)")
        print(f"ç·Šæ€¥é–¾å€¤: {CRITICAL_THRESHOLD * 100:.0f}% ({int(TOKEN_LIMIT * CRITICAL_THRESHOLD):,} tokens)")
        print(f"æŽ¨å®šå¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³/å¯¾è©±: 3,000 tokens")
        print()
        print("âš ï¸  æ³¨æ„:")
        print("   - ã“ã®æŽ¨å®šã¯æ¦‚ç®—ã§ã™")
        print("   - å®Ÿéš›ã®ä½¿ç”¨é‡ã¯å¯¾è©±ã®è¤‡é›‘ã•ã«ã‚ˆã‚Šå¤‰å‹•ã—ã¾ã™")
        print("   - Phase 2ä»¥é™ã§ç²¾åº¦å‘ä¸Šäºˆå®š")
        print()
    
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="VS Code Copilot Chat ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨çŽ‡ç›£è¦–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # åŸºæœ¬ä½¿ç”¨
    python scripts/check_token_usage.py
    
    # è©³ç´°æƒ…å ±è¡¨ç¤º
    python scripts/check_token_usage.py --detailed
        """
    )
    
    parser.add_argument(
        '--detailed',
        action='store_true',
        help='è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º'
    )
    
    args = parser.parse_args()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æŽ¨å®š
    current_usage = estimate_token_usage()
    
    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    print_usage_report(current_usage, detailed=args.detailed)
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    usage_percentage = calculate_usage_percentage(current_usage)
    if usage_percentage >= CRITICAL_THRESHOLD * 100:
        return 2  # ç·Šæ€¥
    elif usage_percentage >= WARNING_THRESHOLD * 100:
        return 1  # è­¦å‘Š
    else:
        return 0  # æ­£å¸¸


if __name__ == '__main__':
    sys.exit(main())
