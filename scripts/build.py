#!/usr/bin/env python3
# Build script: translates content/ja/*.md to content/en/*.md and extracts YAML spec
import os, re, json, yaml, time, shutil
from pathlib import Path
from anthropic import Anthropic

MODEL_DEFAULT = os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
ROOT = Path(__file__).resolve().parents[1]

# ã‚½ãƒ¼ã‚¹è¨€èªã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª
SOURCE_LANG = "ja"
TARGET_LANGS = ["en"]  # å°†æ¥: ["en", "zh", "fr", ...]

# å…¥å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰
CONTENT_DIR = ROOT / "content"
SOURCE_DIR = CONTENT_DIR / SOURCE_LANG
TARGET_DIR = CONTENT_DIR / TARGET_LANGS[0]  # ç¾çŠ¶ã¯enã®ã¿
ROOT_AGENT_MD = ROOT / "AGENT.md"  # è‹±èªç‰ˆã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ

SPEC = ROOT / "spec" / "agent.spec.yaml"
SCHEMA = ROOT / "spec" / "agent.schema.json"
GLOSS = ROOT / "i18n" / "glossary.yml"
PROMPTS = ROOT / "scripts" / "prompts"

# Rate limit: 8,000 output tokens/min
LINES_PER_CHUNK = 100  # è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šãŒä¸æ­£ç¢ºãªãŸã‚ï¼‰
MAX_OUTPUT_TOKENS = 5000  # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™8K/åˆ†ã®ç¯„å›²å†…ï¼‰
RATE_LIMIT_WAIT = 70  # 60ç§’ + ãƒãƒƒãƒ•ã‚¡

def load(p: Path): return p.read_text(encoding="utf-8")
def save(p: Path, s: str):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(s, encoding="utf-8")

def compile_glossary(gloss):
    data = yaml.safe_load(gloss)
    entries = []
    for t in data.get("terms", []):
        if t.get("id") == "personas":
            for it in t.get("items", []):
                entries.append((it["ja"], f'personas:{it["emoji"]}', it["en"]["term"]))
        else:
            entries.append((t["ja"], t["id"], t["en"]["term"]))
    entries.sort(key=lambda x: len(x[0]), reverse=True)
    return entries, data

def inject_anchors(text: str, entries):
    out = text
    for ja, id_, _en in entries:
        out = re.sub(rf'(?<!\{{\#){re.escape(ja)}(?!\}})', f'{ja}{{#{id_}}}', out)
    return out

def estimate_tokens(text: str) -> int:
    """æ–‡å­—æ•°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¦‚ç®—ï¼ˆæ—¥æœ¬èª: 1æ–‡å­—â‰ˆ1.5ãƒˆãƒ¼ã‚¯ãƒ³, è‹±æ•°å­—: 1æ–‡å­—â‰ˆ0.25ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰"""
    # ç°¡æ˜“çš„ã«ã€ç·æ–‡å­—æ•° / 2.5 ã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
    return int(len(text) / 2.5)

def split_document_by_lines(text: str, lines_per_chunk: int = LINES_PER_CHUNK):
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã§å˜ç´”ã«åˆ†å‰²
    ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šãŒä¸æ­£ç¢ºãªãŸã‚ã€ã‚·ãƒ³ãƒ—ãƒ«ãªè¡Œæ•°åˆ†å‰²ã‚’ä½¿ç”¨ï¼‰
    """
    lines = text.split('\n')
    chunks = []

    # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã‚’æŠ½å‡º
    header_lines = []
    content_start = 0
    for i, line in enumerate(lines):
        if line.startswith('## '):
            content_start = i
            break
        header_lines.append(line)

    header = '\n'.join(header_lines)
    content_lines = lines[content_start:]

    # è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²
    for i in range(0, len(content_lines), lines_per_chunk):
        chunk_lines = content_lines[i:i + lines_per_chunk]
        chunks.append('\n'.join(chunk_lines))

    return chunks, header

def chat(model, system, prompt, temperature=0.0):
    client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])
    rsp = client.messages.create(
        model=model,
        max_tokens=MAX_OUTPUT_TOKENS,  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: 8,000 tokens/åˆ†ã‚’å®ˆã‚‹
        temperature=temperature,
        system=system,
        messages=[{"role":"user","content":prompt}],
        timeout=300.0  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    )
    return rsp.content[0].text

def translate_file(source_file: Path, target_file: Path, target_lang: str, glossary_map: dict):
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¿»è¨³
    
    Args:
        source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆcontent/ja/AGENT.mdç­‰ï¼‰
        target_file: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆcontent/en/AGENT.mdç­‰ï¼‰
        target_lang: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆ"en", "zh"ç­‰ï¼‰
        glossary_map: ç”¨èªè¾æ›¸ãƒãƒƒãƒ—
    """
    print(f"[INFO] Translating {source_file.name} ({SOURCE_LANG} â†’ {target_lang})...")
    
    ja = load(source_file)
    gloss = load(GLOSS)
    entries, _ = compile_glossary(gloss)
    ja_anchored = inject_anchors(ja, entries)
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²
    print(f"[INFO] Splitting {source_file.name} into chunks...")
    chunks, header = split_document_by_lines(ja_anchored, LINES_PER_CHUNK)
    print(f"[INFO] Document split into {len(chunks)} chunks (each ~{LINES_PER_CHUNK} lines)")
    
    # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ç¿»è¨³
    sys_trans = load(PROMPTS / "01_en_translate.txt")
    translated_chunks = []
    
    for i, chunk in enumerate(chunks):
        lines = chunk.count('\n') + 1
        print(f"[INFO] Translating chunk {i+1}/{len(chunks)} (~{lines} lines)...")
        
        en_chunk = chat(
            MODEL_DEFAULT,
            sys_trans,
            f"### Glossary Map (id->EN)\n{json.dumps(glossary_map, ensure_ascii=False)}\n\n### JA (anchored)\n{chunk}"
        )
        translated_chunks.append(en_chunk)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ä»¥å¤–ã¯å¾…æ©Ÿ
        if i < len(chunks) - 1:
            print(f"[INFO] Waiting {RATE_LIMIT_WAIT}s for rate limit...")
            time.sleep(RATE_LIMIT_WAIT)
    
    # ç¿»è¨³çµæœã‚’çµåˆ
    print(f"[INFO] Merging translated chunks for {source_file.name}...")
    en_md = translated_chunks[0]
    for chunk in translated_chunks[1:]:
        chunk_lines = chunk.split('\n')
        content_start = 0
        in_header = False
        for j, line in enumerate(chunk_lines):
            if line.startswith('Codename:') or line.startswith('Version:'):
                in_header = True
            elif line.strip() == '---' and in_header:
                content_start = j + 1
                break
        
        if content_start > 0:
            chunk_content = '\n'.join(chunk_lines[content_start:])
        else:
            chunk_content = chunk
        
        en_md += '\n\n' + chunk_content.strip()
    
    # ä¿å­˜
    save(target_file, en_md.strip() + "\n")
    print(f"[INFO] âœ… {source_file.name} â†’ {target_file.name} completed")

def main():
    """
    å¤šè¨€èªç¿»è¨³ + YAMLæŠ½å‡º + ãƒ«ãƒ¼ãƒˆAGENT.mdç”Ÿæˆ
    """
    print("=" * 60)
    print("ğŸ“ Multilingual Build Started")
    print("=" * 60)
    
    # ç”¨èªè¾æ›¸ã‚’èª­ã¿è¾¼ã¿
    gloss = load(GLOSS)
    entries, gloss_obj = compile_glossary(gloss)
    
    # glossary map for translator
    glossary_map = {}
    for t in yaml.safe_load(gloss).get("terms", []):
        if t.get("id") == "personas":
            for it in t.get("items", []):
                glossary_map[f'personas:{it["emoji"]}'] = it["en"]["term"]
        else:
            glossary_map[t["id"]] = t["en"]["term"]
    
    # 1) å¤šè¨€èªç¿»è¨³: ja/ ã®å…¨.mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’ en/ ã«ç¿»è¨³
    print("\n" + "=" * 60)
    print(f"ğŸ“š Step 1: Translating {SOURCE_LANG}/ â†’ {TARGET_LANGS[0]}/")
    print("=" * 60)
    
    TARGET_DIR.mkdir(parents=True, exist_ok=True)
    
    for source_file in SOURCE_DIR.glob("*.md"):
        target_file = TARGET_DIR / source_file.name
        translate_file(source_file, target_file, TARGET_LANGS[0], glossary_map)
    
    # 2) ãƒ«ãƒ¼ãƒˆAGENT.mdã‚’è‹±èªç‰ˆã‹ã‚‰ã‚³ãƒ”ãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ“„ Step 2: Copying content/en/AGENT.md â†’ AGENT.md (entry point)")
    print("=" * 60)
    
    en_agent = TARGET_DIR / "AGENT.md"
    if en_agent.exists():
        shutil.copy(en_agent, ROOT_AGENT_MD)
        print(f"[INFO] âœ… AGENT.md created at root (English entry point)")
    else:
        print(f"[WARN] âš ï¸ content/en/AGENT.md not found, skipping root copy")
    
    # 3) YAML spec (å…ƒã®JAã‹ã‚‰æŠ½å‡º)
    print("\n" + "=" * 60)
    print("ğŸ“¦ Step 3: Extracting YAML spec from content/ja/AGENT.md")
    print("=" * 60)
    
    ja_agent = SOURCE_DIR / "AGENT.md"
    ja = load(ja_agent)
    
    sys_yaml = load(PROMPTS / "02_yaml_extract.txt")
    yaml_out = chat(MODEL_DEFAULT, sys_yaml, f"### JA (truth)\n{ja}")
    
    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
    yaml_clean = yaml_out.strip()
    if yaml_clean.startswith('```'):
        lines = yaml_clean.split('\n')
        yaml_clean = '\n'.join(lines[1:-1])  # æœ€åˆã¨æœ€å¾Œã®```è¡Œã‚’é™¤å»
    
    spec_obj = yaml.safe_load(yaml_clean)
    save(SPEC, yaml.dump(spec_obj, allow_unicode=True, sort_keys=False))
    print(f"[INFO] âœ… spec/agent.spec.yaml generated")
    
    print("\n" + "=" * 60)
    print("âœ… Multilingual Build Completed Successfully!")
    print("=" * 60)
    print(f"\nğŸ“ Generated files:")
    print(f"  - content/{TARGET_LANGS[0]}/*.md (translations)")
    print(f"  - AGENT.md (English entry point)")
    print(f"  - spec/agent.spec.yaml (YAML spec)")

if __name__ == "__main__":
    main()
```
